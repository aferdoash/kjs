{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirection LSTM - IMDB sentiment classification\n",
    "\n",
    "see **https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS_FILEPATH = 'imdb_bidirectional_lstm.hdf5'\n",
    "MODEL_ARCH_FILEPATH = 'imdb_bidirectional_lstm.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n",
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (1000, 200))\n",
      "('X_test shape:', (1000, 200))\n"
     ]
    }
   ],
   "source": [
    "max_features = 20\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 39s - loss: 0.6949 - acc: 0.5030 - val_loss: 0.6926 - val_acc: 0.5140\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 39s - loss: 0.6941 - acc: 0.5090 - val_loss: 0.6931 - val_acc: 0.5080\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 38s - loss: 0.6938 - acc: 0.5020 - val_loss: 0.6927 - val_acc: 0.5280\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 38s - loss: 0.6930 - acc: 0.5060 - val_loss: 0.6925 - val_acc: 0.5250\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6903 - acc: 0.5320 - val_loss: 0.6929 - val_acc: 0.5210\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6874 - acc: 0.5700 - val_loss: 0.6936 - val_acc: 0.4940\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6839 - acc: 0.5600 - val_loss: 0.6934 - val_acc: 0.5130\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6836 - acc: 0.5570 - val_loss: 0.6934 - val_acc: 0.5420\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6787 - acc: 0.5700 - val_loss: 0.6958 - val_acc: 0.5200\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 37s - loss: 0.6778 - acc: 0.5900 - val_loss: 0.6932 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc60878dc50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model saving callback\n",
    "#checkpointer = ModelCheckpoint(filepath=WEIGHTS_FILEPATH, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "# Early stopping\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', verbose=1, patience=2)\n",
    "\n",
    "# train\n",
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=[X_test, y_test],\n",
    "          batch_size=batch_size, nb_epoch=nb_epoch, verbose=1) #,          callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(MODEL_ARCH_FILEPATH, 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: '/home/ubuntu/.keras/datasets/imdb_word_index.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4f4a335c4f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/keras/datasets/imdb.pyc\u001b[0m in \u001b[0;36mget_word_index\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    105\u001b[0m     path = get_file(path,\n\u001b[1;32m    106\u001b[0m                     \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     md5_hash='72d94b01291be4ff843198d3b0e1e4d7')\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, cache_subdir)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 13] Permission denied: '/home/ubuntu/.keras/datasets/imdb_word_index.pkl'"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict = {idx: word for word, idx in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = []\n",
    "for idx in X_train[0]:\n",
    "    if idx >= 3:\n",
    "        sample.append(word_dict[idx-3])\n",
    "    elif idx == 2:\n",
    "        sample.append('-')\n",
    "' '.join(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('imdb_dataset_word_index_top20000.json', 'w') as f:\n",
    "    f.write(json.dumps({word: idx for word, idx in word_index.items() if idx < max_features}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('imdb_dataset_word_dict_top20000.json', 'w') as f:\n",
    "    f.write(json.dumps({idx: word for word, idx in word_index.items() if idx < max_features}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_test_data = []\n",
    "for i in np.random.choice(range(X_test.shape[0]), size=1000, replace=False):\n",
    "    sample_test_data.append({'values': X_test[i].tolist(), 'label': y_test[i].tolist()})\n",
    "    \n",
    "with open('imdb_dataset_test.json', 'w') as f:\n",
    "    f.write(json.dumps(sample_test_data))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
